{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipe import *\n",
    "from functools import partial\n",
    "\n",
    "from research.retina.detector import *\n",
    "from research.optical_flow.focused import *\n",
    "from research.optical_flow.dense import *\n",
    "from research.deep_pipeline.models import *\n",
    "from research.optical_flow.utils import *\n",
    "from research.deep_pipeline.utils import combine_models\n",
    "\n",
    "from utils.image import imprint\n",
    "\n",
    "import os\n",
    "import keras\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import mPyPl as mp\n",
    "from mPyPl.utils.flowutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Load config dictionary\n",
    "config = pickle.load(open('config.pickle', 'rb'))\n",
    "\n",
    "# RetinaFlow setup\n",
    "retina = RetinaNet(config['retinaflow']['path'])\n",
    "flow = FocusedFlow(\n",
    "    config['retinaflow']['edge_detector'], \n",
    "    config['retinaflow']['lucas_kanade']\n",
    ")\n",
    "\n",
    "# VGG16 embeddings setup\n",
    "vgg = keras.applications.vgg16.VGG16(\n",
    "    include_top=False, \n",
    "    weights='imagenet', \n",
    "    input_shape=config['embeddings']['input_shape']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "race-events-recognition/research/deep_pipeline/utils.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  model = Model(input=inputs,output=output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 190, 100, 2)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 125, 7680, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 190, 100, 2)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            952577      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 100)          191748      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 100)          388620      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 201)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            202         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,533,147\n",
      "Trainable params: 1,533,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Combined model setup\n",
    "m1 = RetinaFlowNN(config['retinaflow']['shape'])\n",
    "m2 = EmbeddingsNN(config['embeddings']['output_shape'])\n",
    "m3 = DenseFlowNN(config['denseflow']['shape'])\n",
    "model = combine_models(\n",
    "    [\n",
    "        config['retinaflow']['shape'], \n",
    "        config['embeddings']['output_shape'], \n",
    "        config['denseflow']['shape']\n",
    "    ], \n",
    "    [m1, m2, m3]\n",
    ")\n",
    "model.load_weights(config['combined']['path'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 3.5 s, total: 21.7 s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_file = 'videos/output.mp4'\n",
    "\n",
    "stream = (\n",
    "    mp.videosource_chunked('videos/output2.mp4',video_size=config['retinaflow']['video_size'])\n",
    "    | mp.chunk_slide(5)\n",
    "    | mp.as_field('video')\n",
    "    | mp.apply('video', 'uint_video', lambda x: x.astype(np.uint8))\n",
    "    # RetinaFlow pipeline\n",
    "    | mp.apply('uint_video', 'boxes', lambda x: [retina.predict(frame) for frame in x])\n",
    "    | mp.apply(['uint_video', 'boxes'], 'raw_rflows', \n",
    "               lambda args: flow.run(args[0], args[1], config['retinaflow']['offset']))\n",
    "    | mp.apply('raw_rflows', 'gradients', calc_gradients)\n",
    "    | mp.apply('gradients', 'polar', to_polar)\n",
    "    | mp.apply('polar', 'histograms', partial(video_to_hist,params=config['retinaflow']['hist_params']))\n",
    "    | mp.apply('histograms', 'res_rflows', partial(zero_pad_hist,shape=config['retinaflow']['shape']))\n",
    "    | mp.delfield(['boxes', 'gradients', 'raw_rflows', 'polar', 'histograms'])\n",
    "    # DenseFlow pipeline\n",
    "    | mp.apply('uint_video', 'dflow_video', partial(resize_video,video_size=config['denseflow']['video_size']))\n",
    "    | mp.apply('dflow_video', 'raw_dflows', get_flow_descriptor)\n",
    "    | mp.apply('raw_dflows','padded', partial(mp.zero_pad,max_frames=config['max_frames']))\n",
    "    | mp.apply('padded','res_dflows', partial(\n",
    "        normalize_histograms,\n",
    "        ang_diap=config['denseflow']['ang_diap'],\n",
    "        mag_diap=config['denseflow']['mag_diap']\n",
    "    ))\n",
    "    | mp.delfield(['dflow_video', 'raw_dflows', 'padded'])\n",
    "    # VGG16 embeddings pipeline\n",
    "    | mp.apply('video', 'vgg_video', partial(resize_video,video_size=config['embeddings']['video_size']))\n",
    "    | mp.apply('vgg_video', 'res_vgg', \n",
    "               lambda x: vgg.predict(keras.applications.vgg16.preprocess_input(x)).reshape(config['embeddings']['output_shape']))\n",
    "    | mp.delfield('vgg_video')\n",
    "    # Utils\n",
    "    | mp.apply('res_rflows', 'rflows', partial(np.expand_dims,axis=0))\n",
    "    | mp.apply('res_dflows', 'dflows', partial(np.expand_dims,axis=0))\n",
    "    | mp.apply('res_vgg', 'vgg', partial(np.expand_dims,axis=0))\n",
    "    | mp.delfield(['res_rflows', 'res_dflows', 'res_vgg'])\n",
    "    # Predict\n",
    "    | mp.apply(['rflows', 'vgg', 'dflows'],'score', lambda args: model.predict(args))\n",
    "    | mp.apply(['video','score'],'video_to_display',lambda args: imprint(args[0][50:75],args[1:]))\n",
    "    | mp.select_field('video_to_display')\n",
    "    | mp.collect_video(output_file)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
